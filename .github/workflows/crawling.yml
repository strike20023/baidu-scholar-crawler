name: crawling_web
on:
  repository_dispatch:
    types: [crawling_web]
permissions:
  contents: write
jobs:
  build:
    runs-on: ubuntu-latest
    services:
      selenium:
        image: selenium/standalone-chrome:latest
        options: --shm-size="2g"
        ports:
          - 4444:4444
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: '3.x'
          cache: 'pip'
      
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
      
      - name: Wait for Selenium
        run: |
          timeout 60 bash -c 'until curl -s http://localhost:4444/wd/hub/status > /dev/null; do sleep 2; done'
      
      - name: Run main.py
        env:
          QUERY: ${{ github.event.client_payload.query }}
        run: python main.py
      
      - name: Upload output
        uses: actions/upload-artifact@v4
        with:
          name: crawled-data-${{ github.event.client_payload.query }}
          path: |
            crawled-data-*.json
          retention-days: 30

      - name: Commit and push outputs
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add outputs/
          git commit -m "Add crawled data for query: ${{ github.event.client_payload.query }}" || echo "No changes to commit"
          git push